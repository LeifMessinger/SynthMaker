{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from synth import Synth, Wave\n",
    "from synth_generator import WaveIterableDataset\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "dataset = WaveIterableDataset(batch_size=100, duration=2.0, sample_rate=48000)\n",
    "dataloader = DataLoader(dataset, batch_size=None)  \n",
    "\n",
    "class AudioFeatureExtractor(nn.Module):\n",
    "    def __init__(self, sample_rate=48000, n_fft=2048, n_mels=128):\n",
    "        super().__init__()\n",
    "        self.mel_spectrogram = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=512,\n",
    "            n_mels=n_mels,\n",
    "            normalized=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mel_spec = self.mel_spectrogram(x)\n",
    "        return mel_spec\n",
    "\n",
    "\n",
    "class SynthParameterPredictor(nn.Module):\n",
    "    def __init__(self, input_dim=128, hidden_dim=256, output_dim=3):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "                \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            #nn.Linear(64 * (input_dim // 8) * 93, hidden_dim),  \n",
    "            nn.LazyLinear(hidden_dim),\n",
    "            #nn.Linear(64 * 16 * 23, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim // 2, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        #print(f\"Shape before flattening: {x.shape}\")\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "feature_extractor = AudioFeatureExtractor().to(device)\n",
    "model = SynthParameterPredictor().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_batches = 5  \n",
    "total_loss = 0\n",
    "batch_losses = []\n",
    "\n",
    "param_ranges = {\n",
    "    'frequency': (110, 880),    #A2 to A5\n",
    "    'phase': (0, 1),\n",
    "    'volume': (0.2, 1.0)\n",
    "}\n",
    "\n",
    "def normalize_params(params):\n",
    "    \"\"\"Normalize parameters to [0, 1] range\"\"\"\n",
    "    normalized = torch.zeros_like(params)\n",
    "    normalized[:, 0] = (params[:, 0] - 110) / (880 - 110)  \n",
    "    normalized[:, 1] = params[:, 1]  \n",
    "    normalized[:, 2] = (params[:, 2] - 0.2) / (1.0 - 0.2)  \n",
    "    return normalized\n",
    "\n",
    "def denormalize_params(norm_params):\n",
    "    \"\"\"Convert normalized parameters back to original range\"\"\"\n",
    "    denorm = torch.zeros_like(norm_params)\n",
    "    denorm[:, 0] = norm_params[:, 0] * (880 - 110) + 110  \n",
    "    denorm[:, 1] = norm_params[:, 1]  \n",
    "    denorm[:, 2] = norm_params[:, 2] * (1.0 - 0.2) + 0.2  \n",
    "    return denorm\n",
    "\n",
    "\n",
    "model.train()\n",
    "for batch_idx, (audio_batch, params_batch) in enumerate(dataloader):\n",
    "    if batch_idx >= num_batches:\n",
    "        break\n",
    "            \n",
    "    audio_batch = audio_batch.to(device)\n",
    "    params_batch = params_batch.to(device)\n",
    "    \n",
    "    normalized_params = normalize_params(params_batch)\n",
    "    \n",
    "    #with torch.no_grad():\n",
    "    features = feature_extractor(audio_batch).unsqueeze(1)  #Add channel dimension\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(features)\n",
    "    loss = criterion(predictions, normalized_params)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    batch_loss = loss.item()\n",
    "    batch_losses.append(batch_loss)\n",
    "    \n",
    "    print(f\"Batch {batch_idx+1}/{num_batches}, Loss: {batch_loss:.6f}\")\n",
    "    \n",
    "    if batch_idx == num_batches - 1:  #Last batch\n",
    "        model.eval()\n",
    "\n",
    "        #with torch.no_grad():\n",
    "        sample_indices = torch.randint(0, len(audio_batch), (5,))\n",
    "        sample_audio = audio_batch[sample_indices]\n",
    "        sample_params = params_batch[sample_indices]\n",
    "        \n",
    "        sample_features = feature_extractor(sample_audio).unsqueeze(1)\n",
    "        sample_predictions = model(sample_features)\n",
    "        \n",
    "        denorm_predictions = denormalize_params(sample_predictions)\n",
    "        \n",
    "        print(\"\\nSample Predictions:\")\n",
    "        print(\"Index | Parameter | True Value | Predicted Value\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        param_names = ['Frequency', 'Phase', 'Volume']\n",
    "        for i, (true, pred) in enumerate(zip(sample_params, denorm_predictions)):\n",
    "            print(f\"Sample {i+1}:\")\n",
    "            for j, name in enumerate(param_names):\n",
    "                print(f\"  {name}: {true[j]:.4f} | {pred[j]:.4f}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_batches+1), batch_losses, marker='o')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, 'synth_parameter_predictor.pth')\n",
    "\n",
    "print(\"Model saved to 'synth_parameter_predictor.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize spectrograms and predictions\n",
    "def visualize_prediction(audio, true_params, pred_params):\n",
    "    \"\"\"Visualize the audio spectrogram and the parameter predictions\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot spectrogram\n",
    "    plt.subplot(2, 1, 1)\n",
    "    spec = feature_extractor(audio.unsqueeze(0)).squeeze().cpu().numpy()\n",
    "    plt.imshow(spec, aspect='auto', origin='lower', cmap='viridis')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Mel Spectrogram')\n",
    "    plt.xlabel('Time Frames')\n",
    "    plt.ylabel('Mel Frequency Bins')\n",
    "    \n",
    "    # Plot parameter comparison\n",
    "    plt.subplot(2, 1, 2)\n",
    "    param_names = ['Frequency (Hz)', 'Phase (0-1)', 'Volume (0-1)']\n",
    "    x = np.arange(len(param_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    true_values = true_params.cpu().numpy()\n",
    "    pred_values = pred_params.cpu().numpy()\n",
    "    \n",
    "    plt.bar(x - width/2, true_values, width, label='True')\n",
    "    plt.bar(x + width/2, pred_values, width, label='Predicted')\n",
    "    \n",
    "    plt.xticks(x, param_names)\n",
    "    plt.ylabel('Parameter Value')\n",
    "    plt.title('Parameter Comparison')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a few examples\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get a new batch of data\n",
    "    audio_batch, params_batch = next(iter(dataloader))\n",
    "    audio_batch = audio_batch.to(device)\n",
    "    params_batch = params_batch.to(device)\n",
    "    \n",
    "    # Extract features and predict\n",
    "    features = feature_extractor(audio_batch).unsqueeze(1)\n",
    "    normalized_predictions = model(features)\n",
    "    predictions = denormalize_params(normalized_predictions)\n",
    "    \n",
    "    # Visualize 2 random examples\n",
    "    sample_indices = torch.randint(0, len(audio_batch), (2,))\n",
    "    for idx in sample_indices:\n",
    "        visualize_prediction(\n",
    "            audio_batch[idx], \n",
    "            params_batch[idx], \n",
    "            predictions[idx]\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
