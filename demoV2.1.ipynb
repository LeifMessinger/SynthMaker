{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from synth import Synth, Wave\n",
    "from synth_generator import WaveIterableDataset\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded. Skipping training.\n"
     ]
    }
   ],
   "source": [
    "dataset = WaveIterableDataset(duration=2.0, sample_rate=48000)\n",
    "import multiprocessing\n",
    "dataloader = DataLoader(\n",
    "\tdataset,\n",
    "\tbatch_size=100, \n",
    "\tnum_workers=4,\n",
    "\tmultiprocessing_context=multiprocessing.get_context('spawn')\n",
    ")\n",
    "\n",
    "class AudioFeatureExtractor(nn.Module):\n",
    "\tdef __init__(self, sample_rate=48000, n_fft=2048, n_mels=128):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.mel_spectrogram = T.MelSpectrogram(\n",
    "\t\t\tsample_rate=sample_rate,\n",
    "\t\t\tn_fft=n_fft,\n",
    "\t\t\thop_length=512,\n",
    "\t\t\tn_mels=n_mels,\n",
    "\t\t\tnormalized=True\n",
    "\t\t)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tmel_spec = self.mel_spectrogram(x)\n",
    "\t\treturn mel_spec\n",
    "\n",
    "\n",
    "class SynthParameterPredictor(nn.Module):\n",
    "\tdef __init__(self, input_dim=128, hidden_dim=256, output_dim=3):\n",
    "\t\tsuper().__init__()\n",
    "\t\t\t\t\n",
    "\t\tself.conv_layers = nn.Sequential(\n",
    "\t\t\t#Conv layers\n",
    "\t\t\tnn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool2d(2),\n",
    "\t\t\tnn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool2d(2),\n",
    "\t\t\tnn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool2d(2),\n",
    "\n",
    "\t\t\t#Linear layers\n",
    "\t\t\tnn.Flatten(),\n",
    "\t\t\tnn.LazyLinear(hidden_dim),\n",
    "\t\t\t#nn.Linear(64 * 16 * 23, hidden_dim),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(0.3),\n",
    "\t\t\tnn.Linear(hidden_dim, hidden_dim // 2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(0.3),\n",
    "\t\t\tnn.Linear(hidden_dim // 2, output_dim)\n",
    "\t\t)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.conv_layers(x)\n",
    "\n",
    "\n",
    "feature_extractor = AudioFeatureExtractor().to(device)\n",
    "model = SynthParameterPredictor().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "checkpoint_path = 'synth_parameter_predictor.pth'\n",
    "import os\n",
    "if os.path.exists(checkpoint_path):\n",
    "\tcheckpoint = torch.load(checkpoint_path)\n",
    "\tmodel.load_state_dict(checkpoint['model_state_dict'])\n",
    "\toptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\tmodel.eval()\n",
    "\tprint(\"Checkpoint loaded. Skipping training.\")\n",
    "\tloadedModel = True\n",
    "else:\n",
    "\tprint(\"Checkpoint not found. Proceeding with training.\")\n",
    "\tloadedModel = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 200\n",
    "total_loss = 0\n",
    "batch_losses = []\n",
    "\n",
    "param_ranges = {\n",
    "\t'frequency': (110, 880),    #A2 to A5\n",
    "\t'phase': (0, 1),\n",
    "\t'volume': (0.2, 1.0)\n",
    "}\n",
    "\n",
    "def normalize_params(params):\n",
    "\t\"\"\"Normalize parameters to [0, 1] range\"\"\"\n",
    "\tnormalized = torch.zeros_like(params)\n",
    "\tnormalized[0] = (params[0] - 110) / (880 - 110)  \n",
    "\tnormalized[1] = params[1]  \n",
    "\tnormalized[2] = (params[2] - 0.2) / (1.0 - 0.2)  \n",
    "\treturn normalized\n",
    "\n",
    "def normalize_batch(batch):\n",
    "\treturn torch.stack([normalize_params(params) for params in batch])\n",
    "\n",
    "def denormalize_params(norm_params):\n",
    "\t\"\"\"Convert normalized parameters back to original range\"\"\"\n",
    "\tdenorm = torch.zeros_like(norm_params)\n",
    "\tdenorm[0] = norm_params[0] * (880 - 110) + 110  \n",
    "\tdenorm[1] = norm_params[1]  \n",
    "\tdenorm[2] = norm_params[2] * (1.0 - 0.2) + 0.2  \n",
    "\treturn denorm\n",
    "\n",
    "def denormalize_batch(batch):\n",
    "\treturn torch.stack([denormalize_params(params) for params in batch])\n",
    "\n",
    "if not loadedModel:\n",
    "\tmodel.train()\n",
    "\tfor batch_idx in range(0, num_batches):\n",
    "\t\taudio_batch, params_batch = next(iter(dataloader))\n",
    "\t\t\t\t\n",
    "\t\taudio_batch = audio_batch.to(device)\n",
    "\t\tparams_batch = params_batch.to(device)\n",
    "\t\t\n",
    "\t\tnormalized_params = normalize_batch(params_batch).to(device)\n",
    "\t\t\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfeatures = feature_extractor(audio_batch)\n",
    "\t\t\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tpredictions = model(features.unsqueeze(1))\n",
    "\t\tloss = criterion(predictions, normalized_params)\n",
    "\t\t\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t\n",
    "\t\tbatch_loss = loss.item()\n",
    "\t\tbatch_losses.append(batch_loss)\n",
    "\t\t\n",
    "\t\tprint(f\"Batch {batch_idx+1}/{num_batches}, Loss: {batch_loss:.6f}\")\n",
    "\t\t\n",
    "\t\tif batch_idx == num_batches - 1:  #Last batch\n",
    "\t\t\tmodel.eval()\n",
    "\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tsample_indices = torch.randint(0, len(audio_batch), (5,))\n",
    "\t\t\t\tsample_audio = audio_batch[sample_indices]\n",
    "\t\t\t\tsample_params = params_batch[sample_indices]\n",
    "\t\t\t\t\n",
    "\t\t\t\tsample_features = feature_extractor(sample_audio).unsqueeze(1)\n",
    "\t\t\t\tsample_predictions = model(sample_features)\n",
    "\n",
    "\t\t\t\tprint(\"Shape of audio:\", sample_audio.shape)\n",
    "\t\t\t\tprint(\"Shape of features:\", sample_features.shape)\n",
    "\t\t\t\t\n",
    "\t\t\t\tdenorm_predictions = denormalize_batch(sample_predictions).detach().cpu().numpy()\n",
    "\t\t\t\t\n",
    "\t\t\t\tprint(\"\\nSample Predictions:\")\n",
    "\t\t\t\tprint(\"Index | Parameter | True Value | Predicted Value\")\n",
    "\t\t\t\tprint(\"-\" * 50)\n",
    "\t\t\t\t\n",
    "\t\t\t\tparam_names = ['Frequency', 'Phase', 'Volume']\n",
    "\t\t\t\tfor i, (true, pred) in enumerate(zip(sample_params, denorm_predictions)):\n",
    "\t\t\t\t\tprint(f\"Sample {i+1}:\")\n",
    "\t\t\t\t\tfor j, name in enumerate(param_names):\n",
    "\t\t\t\t\t\tprint(f\"  {name}: {true[j]:.4f} | {pred[j]:.4f}\")\n",
    "\n",
    "\n",
    "\tplt.figure(figsize=(10, 5))\n",
    "\tplt.plot(range(1, num_batches+1), batch_losses, marker='o')\n",
    "\tplt.title('Training Loss')\n",
    "\tplt.xlabel('Batch')\n",
    "\tplt.ylabel('Loss')\n",
    "\tplt.grid(True)\n",
    "\tplt.show()\n",
    "\n",
    "\ttorch.save({\n",
    "\t\t'model_state_dict': model.state_dict(),\n",
    "\t\t'optimizer_state_dict': optimizer.state_dict(),\n",
    "\t}, 'synth_parameter_predictor.pth')\n",
    "\tprint(\"Model saved to 'synth_parameter_predictor.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Avg Loss = 0.316668, Param MSE = 1426835.500000\n",
      "Epoch 10: Avg Loss = 0.322952, Param MSE = 1024361.500000\n",
      "Epoch 20: Avg Loss = 0.431762, Param MSE = 2630281.000000\n",
      "Epoch 30: Avg Loss = 0.421652, Param MSE = 230952.921875\n",
      "Epoch 40: Avg Loss = 0.370940, Param MSE = 1667228.500000\n",
      "Epoch 50: Avg Loss = 0.346912, Param MSE = 1175408.500000\n",
      "Epoch 60: Avg Loss = 0.277563, Param MSE = 417643.718750\n",
      "Epoch 70: Avg Loss = 0.537395, Param MSE = 1429459.250000\n",
      "Epoch 80: Avg Loss = 0.434282, Param MSE = 2373.382080\n",
      "Epoch 90: Avg Loss = 0.477019, Param MSE = 36297.312500\n",
      "Training completed!\n",
      "Before parameters: tensor([203.4914,   0.4103,   0.6460], device='cuda:0')\n",
      "True parameters: [258.7986       0.8389284    0.80834633]\n",
      "Predicted parameters: [203.49144      0.41026366   0.6459962 ]\n"
     ]
    }
   ],
   "source": [
    "from reinforcement_learner import SynthRL\n",
    "\n",
    "trash_model = model\n",
    "\n",
    "dataset = WaveIterableDataset()\n",
    "model = SynthRL(device, dataset)\n",
    "\n",
    "model.train(num_epochs=100, samples_per_epoch=5, steps_per_sample=10)\n",
    "\n",
    "test_audio, true_params = next(iter(dataset))\n",
    "test_audio = test_audio.to(device)\n",
    "true_params = true_params.to(device)\n",
    "\n",
    "predicted_params = denormalize_params(\n",
    "\ttrash_model(\n",
    "\t\tfeature_extractor(test_audio).unsqueeze(0).unsqueeze(0)\n",
    "\t).squeeze(0)\n",
    ")\n",
    "\n",
    "predicted_params = model.predict(\n",
    "\ttest_audio,\n",
    "\tparams=predicted_params\n",
    ")\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"True parameters:\", true_params.cpu().numpy())\n",
    "print(\"Predicted parameters:\", predicted_params.cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
